{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting the figure size to a bigger one than the default\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['legend.fontsize'] = 16\n",
    "rcParams['axes.labelsize'] = 16\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score \n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read about the dataset here https://samyzaf.com/ML/song_year/song_year.html\n",
    "\n",
    "# Load the data from the file\n",
    "features = ['year', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15', 't16', 't17', 't18', 't1 9', 't20', 't21', 't22', 't23', 't24', 't25', 't26', 't27', 't28', 't29', 't30', 't31', 't32', 't33', 't34', 't35', 't36 ', 't37', 't38', 't39', 't40', 't41', 't42', 't43', 't44', 't45', 't46', 't47', 't48', 't49', 't50', 't51', 't52', 't53' , 't54', 't55', 't56', 't57', 't58', 't59', 't60', 't61', 't62', 't63', 't64', 't65', 't66', 't67', 't68', 't69', 't70', 't71', 't72', 't73', 't74', 't75', 't76', 't77', 't78', 't79', 't80', 't81', 't82', 't83', 't84', 't85', 't86', 't87', 't88', 't89', 't90']\n",
    "\n",
    "data = pd.read_csv('/Users/ignaciopastorebenaim/Documents/MGRCV/TPs/ML/Lab1/data/YearPredictionMSD.csv', header=0, names=features)\n",
    "\n",
    "# First look at the dataset\n",
    "print(data.info())  \n",
    "print(data.describe())  \n",
    "print(data.head())\n",
    "\n",
    "# If you want to see the total number of missing values in the entire dataset\n",
    "total_missing_values = data.isna().sum().sum()\n",
    "print(f\"Total number of missing values in the dataset: {total_missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to numpy array\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = data[:, 1:]\n",
    "y = data[:, 0]   \n",
    "\n",
    "# Split the data in training, validation, and test sets\n",
    "X_train_and_val, X_test, y_train_and_val, y_test = train_test_split(X, y, test_size=0.1, random_state=5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_and_val, y_train_and_val, test_size=0.1, random_state=7)\n",
    "\n",
    "print('# training samples: ', X_train.shape[0])\n",
    "print('# validation samples: ', X_val.shape[0])\n",
    "print('# test samples: ', X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation between each feature in X_train and y_train\n",
    "correlations = []\n",
    "for i in range(X_train.shape[1]):\n",
    "  corr, _ = pearsonr(X_train[:, i], y_train)\n",
    "  correlations.append(corr)\n",
    "\n",
    "# Plot the correlations\n",
    "plt.bar(range(len(correlations)), correlations)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Correlation with Output')\n",
    "plt.title('Correlation of Each Feature with the Output')\n",
    "plt.show()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = np.corrcoef(X_train, rowvar=False)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Filter correlations with a threshold of 0.1\n",
    "correlations = np.array(correlations)\n",
    "threshold = 0.1\n",
    "filtered_indices = np.where(np.abs(correlations) >= threshold)[0]\n",
    "filtered_correlations = correlations[filtered_indices]\n",
    "\n",
    "print(\"Filtered correlations: \", filtered_correlations)\n",
    "print(\"Filtered indexes: \", filtered_indices)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, feature_index in enumerate(filtered_indices):\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  plt.scatter(X_train[:, feature_index], y_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the pipeline with PolynomialFeatures and PCA\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('oversampler', RandomOverSampler(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures()),  # Add polynomial features step\n",
    "    ('pca', PCA()),  # Optionally add PCA for dimensionality reduction\n",
    "    ('model', Ridge())  # Temporary model placeholder for RandomizedSearch\n",
    "])\n",
    "\n",
    "# Define the parameter grid for multiple models and degrees of polynomial features\n",
    "param_grid = [\n",
    "    {\n",
    "        'poly__degree': [1, 2, 3],\n",
    "        'pca__n_components': [None, 0.90, 0.95, 0.99],  # Adding None to make PCA optional\n",
    "        'model': [Ridge()],\n",
    "        'model__alpha': [0.1, 1.0, 10.0]  # Different Ridge regression alpha values\n",
    "    },\n",
    "    {\n",
    "        'poly__degree': [1, 2, 3],\n",
    "        'pca__n_components': [None, 0.90, 0.95, 0.99],\n",
    "        'model': [Lasso()],\n",
    "        'model__alpha': [0.1, 1.0, 10.0]  # Lasso regularization strength\n",
    "    },\n",
    "    # Additional model configurations...\n",
    "]\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring_metrics = {\n",
    "    'MSE': 'neg_mean_squared_error',       # Mean Squared Error\n",
    "    'MAE': 'neg_mean_absolute_error',      # Mean Absolute Error\n",
    "    'R2': 'r2',                            # RÂ² Score\n",
    "    'MedAE': 'neg_median_absolute_error',  # Median Absolute Error\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45041d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(pipeline, param_grid, n_iter=50, cv=5, scoring=scoring_metrics, refit='MAE', verbose=3, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and hyperparameters\n",
    "print(\"Best model:\", random_search.best_estimator_)\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
