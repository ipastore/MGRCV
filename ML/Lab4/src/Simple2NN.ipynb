{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJA0eFCjcmaI"
   },
   "source": [
    "## LAB 2 - TASK 1 submission. ML 2023-24.\n",
    "FILL UP THIS BOX WITH YOUR DETAILS\n",
    "\n",
    "**NAME AND NIP**: ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dh28l6n1khl"
   },
   "source": [
    "**SOURCE: This colab is based on the materials from Stanford Course - CS231CNNs for Visual Recognition with CNN by Fei-Fei Li**\n",
    "\n",
    "You should **run this Colab in an instance WITHOUT GPU**! (recommended to save your GPU time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N69tTMVBX_Ul"
   },
   "source": [
    "\n",
    "# Implementing a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CgWl7amS1YOe"
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE CLASS for the 2-layer NN\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network that performs classification over C classes.\n",
    "  The network has the following architecture:\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "  Input dimension of N, hidden layer dimension of H.\n",
    "  Training uses a softmax loss function and L2 regularization (on the weights).\n",
    "\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights to small random values and biases to zero.\n",
    "    Weights and biases are stored in the variable self.params,\n",
    "    which is a dictionary with the following keys:\n",
    "\n",
    "    W1: First layer weights; has shape (D, H)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (H, C)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "\n",
    "    Inputs to configure our network:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C.\n",
    "      If it is None we only return scores; else return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    If y is None, a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "\n",
    "    If y is not None, return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function (with the same keys as self.params)\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "\n",
    "    ## BEGINING OF FORWARD PASS ##\n",
    "    # Q2. What's done in this block? (one line comment)\n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "    fc1 = X.dot(W1) + b1     # type of layer? fully connected\n",
    "    X2 = np.maximum(0, fc1)  # type of layer? ReLU\n",
    "    scores = X2.dot(W2) + b2 # type of layer? fully connected\n",
    "\n",
    "    # If labels are not given skip the rest\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # Q3. What's done in this block? (one line comment)\n",
    "    # Compute the loss (data loss - Softmax and L2 regularization)\n",
    "    loss = None\n",
    "    scores -= np.max(scores, axis=1, keepdims=True) # avoid numeric instability\n",
    "    scores_exp = np.exp(scores)\n",
    "    softmax_matrix = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n",
    "    loss = np.sum(-np.log(softmax_matrix[np.arange(N), y]))\n",
    "    loss /= N\n",
    "    loss += reg * (np.sum(W2 * W2) + np.sum( W1 * W1 ))\n",
    "\n",
    "    ## BEGINING OF BACKWARD PASS ##\n",
    "    # Backward pass: compute gradients (derivatives of the weights and biases).\n",
    "    # Fill up grads dictionary,\n",
    "    # e.g., grads['W1'] is the gradient on W1 and both have the same size\n",
    "    grads = {}\n",
    "    softmax_matrix[np.arange(N) ,y] -= 1\n",
    "    softmax_matrix /= N\n",
    "    # W2 gradient\n",
    "    dW2 = X2.T.dot(softmax_matrix)   # [HxN] * [NxC] = [HxC]\n",
    "    # b2 gradient\n",
    "    db2 = softmax_matrix.sum(axis=0)\n",
    "    # W1 gradient\n",
    "    dW1 = softmax_matrix.dot(W2.T)   # [NxC] * [CxH] = [NxH]\n",
    "    dfc1 = dW1 * (fc1>0)             # [NxH] . [NxH] = [NxH]\n",
    "    dW1 = X.T.dot(dfc1)              # [DxN] * [NxH] = [DxH]\n",
    "    # b1 gradient\n",
    "    db1 = dfc1.sum(axis=0)\n",
    "    # regularization gradient\n",
    "    dW1 += reg * 2 * W1\n",
    "    dW2 += reg * 2 * W2\n",
    "\n",
    "    grads = {'W1':dW1, 'b1':db1, 'W2':dW2, 'b2':db2}\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=5e-6, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Q4. WHAT IS THIS FUNCTION DOING?\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in range(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # Q5. What's being done in this block? (one line comment)\n",
    "      # random minibatch of training data and labels in X_batch and y_batch\n",
    "      batch_indices = np.random.choice(num_train, batch_size)\n",
    "      X_batch = X[batch_indices]\n",
    "      y_batch = y[batch_indices]\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # Q6. What's being done in this block? (one line comment)\n",
    "      # With the gradients in the grads dictionary, update the network params using SGD.\n",
    "      for key in self.params:\n",
    "        self.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "    y_pred = None\n",
    "    #y_pred = np.argmax( self.function1(X), axis=1)\n",
    "    y_pred = np.argmax( self.loss(X), axis=1)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRE_o4MX4rRF"
   },
   "source": [
    "# Using the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T2FcNszX_U6"
   },
   "source": [
    "# Load a real dataset\n",
    "Now train the network on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fi8YU7Iy4w5I"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset\n",
    "    data_cifar10 = cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = data_cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bVy2AZiTX_U7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 7s 0us/step\n",
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XRC0R_N25zwD"
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Pb2q3TEAX_U-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.302951\n",
      "iteration 100 / 1000: loss 2.302613\n",
      "iteration 200 / 1000: loss 2.297164\n",
      "iteration 300 / 1000: loss 2.266223\n",
      "iteration 400 / 1000: loss 2.165487\n",
      "iteration 500 / 1000: loss 2.121131\n",
      "iteration 600 / 1000: loss 2.073803\n",
      "iteration 700 / 1000: loss 2.060476\n",
      "iteration 800 / 1000: loss 2.015558\n",
      "iteration 900 / 1000: loss 2.001878\n",
      "Validation accuracy:  0.276\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifUFbbDoX_VA"
   },
   "source": [
    "# Debug the training\n",
    "With the default parameters provided above, you should get a validation accuracy of about 0.20 on the validation set.\n",
    "\n",
    "This isn't very good. Keep reading to improve the training.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yk5whx1X_VB"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNejyFS1X_VG"
   },
   "outputs": [],
   "source": [
    "from math import sqrt, ceil\n",
    "import numpy as np\n",
    "\n",
    "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
    "  \"\"\"\n",
    "  Reshape a 4D tensor of image data to a grid for easy visualization.\n",
    "\n",
    "  Inputs:\n",
    "  - Xs: Data of shape (N, H, W, C)\n",
    "  - ubound: Output grid will have values scaled to the range [0, ubound]\n",
    "  - padding: The number of blank pixels between elements of the grid\n",
    "  \"\"\"\n",
    "  (N, H, W, C) = Xs.shape\n",
    "  grid_size = int(ceil(sqrt(N)))\n",
    "  grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "  grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "  grid = np.zeros((grid_height, grid_width, C))\n",
    "  next_idx = 0\n",
    "  y0, y1 = 0, H\n",
    "  for y in range(grid_size):\n",
    "    x0, x1 = 0, W\n",
    "    for x in range(grid_size):\n",
    "      if next_idx < N:\n",
    "        img = Xs[next_idx]\n",
    "        low, high = np.min(img), np.max(img)\n",
    "        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
    "        # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "        next_idx += 1\n",
    "      x0 += W + padding\n",
    "      x1 += W + padding\n",
    "    y0 += H + padding\n",
    "    y1 += H + padding\n",
    "  # grid_max = np.max(grid)\n",
    "  # grid_min = np.min(grid)\n",
    "  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
    "  return grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqlKANeiX_VI"
   },
   "source": [
    "# Tune your hyperparameters\n",
    "\n",
    "**What can be wrong?**. In the visualizations above we see that the loss is decreasing more or less linearly which could be do to a learning rate too low. Besides, there is no significant gap between the training and validation accuracy, suggesting that the model used has low capacity, and we could increase its size. However, we need to be careful with very large models, where we would expect to see more overfitting. This can manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks.\n",
    "To try to improve the results, next you will:\n",
    "- run the training for more iterations (now we use 2000 instead of 1000)\n",
    "- experiment with different values of  hidden layer size, learning rate and regularization strength.\n",
    "\n",
    "Expect to achieve at most around 50% on the validation set. It's ok if you don't get to that. Just explain which hyperparameters you consider and why in the following question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-ETXnJbBMoy"
   },
   "source": [
    "\n",
    "### **QUESTION 1** What list of possible values for lr, h and reg do you consider? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ph1b2IVeIP9"
   },
   "source": [
    "ANSWER 1: [PUT YOUR ANSWER HERE] (max 4 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSjIU_MXX_VJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code explores different sets of hyperparameters and stores the best configuration\n",
    "\n",
    "best_net = None # store the best model into this\n",
    "best_val = -1\n",
    "best_stats = []\n",
    "\n",
    "# generate random hyperparameters given ranges for each of them\n",
    "def generate_random_hyperparams(lr_min, lr_max, reg_min, reg_max, h_min, h_max):\n",
    "    lr = 10**np.random.uniform(lr_min,lr_max)\n",
    "    reg = 10**np.random.uniform(reg_min,reg_max)\n",
    "    hidden = np.random.randint(h_min, h_max)\n",
    "    return lr, reg, hidden\n",
    "\n",
    "# get random hyperparameters given arrays of potential values\n",
    "def random_search_hyperparams(lr_values, reg_values, h_values):\n",
    "    lr = lr_values[np.random.randint(0,len(lr_values))]\n",
    "    reg = reg_values[np.random.randint(0,len(reg_values))]\n",
    "    hidden = h_values[np.random.randint(0,len(h_values))]\n",
    "    return lr, reg, hidden\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "############### ************* TO-DO-IN-LAB ************* ###############\n",
    "num_random_iterations = 5   #When you have your lists of possible values ready, increase this to 20\n",
    "lr_value_list = [0.0001]    # This is the initial value, add a list with up to 3 possible values\n",
    "str_value_list = [0.25]     # This is the initial value, add a list with up to 3 possible values\n",
    "h_value_list = [50]         # This is the initial value, add a list with up to 3 possible values\n",
    "############### ************* END of TO-DO-IN-LAB ************* ###############\n",
    "\n",
    "# Set a seed for results reproduction\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use of random search for hyperparameter search\n",
    "for i in range(num_random_iterations):\n",
    "    lr, reg, hidden_size = random_search_hyperparams(lr_value_list, str_value_list, h_value_list)\n",
    "\n",
    "    # Create a two-layer network\n",
    "    net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # Train the network\n",
    "    stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                num_iters=2000, batch_size=200,\n",
    "                learning_rate=lr, learning_rate_decay=0.95,\n",
    "                reg=reg, verbose=False)\n",
    "\n",
    "    # Predict on the training set\n",
    "    train_accuracy = (net.predict(X_train) == y_train).mean()\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_accuracy = (net.predict(X_val) == y_val).mean()\n",
    "\n",
    "    # Save best values\n",
    "    if val_accuracy > best_val:\n",
    "        best_val = val_accuracy\n",
    "        best_net = net\n",
    "        best_stats = stats\n",
    "\n",
    "    # Print results\n",
    "    print('lr %e reg %e hid %d  train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, hidden_size, train_accuracy, val_accuracy))\n",
    "print('best validation accuracy achieved: %f' % best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLyfKX21X_VL"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(best_stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(best_stats['train_acc_history'], label='train')\n",
    "plt.plot(best_stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybcjv6mvX_VO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4xhHw-4X_VQ"
   },
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_omF2fLX_VR"
   },
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Rz4aAUX_VT"
   },
   "source": [
    "### **QUESTION 2**\n",
    "When you find that your testing accuracy is much lower than the training accuracy. What can you do to try to fix it? Select all that apply and explain why (max 4 lines).\n",
    "1. Train on a larger dataset.\n",
    "2. Add more hidden units.\n",
    "3. Increase the regularization strength.\n",
    "4. Start with very low values of learning rate\n",
    "5. None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InO3QKmufaJz"
   },
   "source": [
    "ANSWER 2: [YOUR ANSWER HERE]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1M3TTDrvtqdxPeKM9ZhXB_R-APJUfkKHF",
     "timestamp": 1605036509876
    },
    {
     "file_id": "1AspUbuTtEGGTwTEfLYhZQ1SpYFgsPpZx",
     "timestamp": 1605023266474
    }
   ]
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
