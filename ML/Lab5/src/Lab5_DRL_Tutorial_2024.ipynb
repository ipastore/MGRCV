{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKwpmSSxvlK2"
   },
   "source": [
    "##Â LAB 5 - TASK 4 submission. ML 2024-25.\n",
    "**Deep Reinforcement Learning to play ATARI games**\n",
    "\n",
    "\n",
    "FILL UP THIS BOX WITH YOUR DETAILS\n",
    "\n",
    "**NAME AND NIP**: \n",
    "\n",
    "- Ignacio Pastore Benaim, 920576\n",
    "- David Padilla Orenga, 946874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzzbqc-JS2z9"
   },
   "source": [
    "## Deep RL example with ATARI\n",
    "This colab is a tutorial built following materials published by [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx) at [class t81-558](https://sites.wustl.edu/jeffheaton/t81-558/). **Atari Games with Stable Baselines Neural Networks** [[Notebook]](t81_558_class_12_4_atari.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "## Colab setup (run only first time)\n",
    "\n",
    "Install necessary libraries. You may need to re-start the colab environment after all the installation steps have finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9KQhYThvTCQC"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "  !pip install stable-baselines3[extra] gymnasium\n",
    "  !pip install gymnasium[atari]\n",
    "  !pip install pyvirtualdisplay\n",
    "  !sudo apt-get install -y python-opengl ffmpeg\n",
    "  !sudo apt-get install -y xvfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "## Gymnasium Atari Breakout\n",
    "\n",
    "In the context of artificial intelligence research and particularly within reinforcement learning, Atari Breakout has been adapted as an environment within **OpenAI's Gym toolkit**, a collection of environments that provide a standardized interface for algorithm development and benchmarking.\n",
    "\n",
    "**Stable Baselines** is a set of implementations of reinforcement learning algorithms to train and evaluate agents on various tasks, including playing Atari games like Breakout.\n",
    "\n",
    "The adaptation of **Breakout game** to the Gym environment, often referred to as 'Breakout-v0' or 'BreakoutDeterministic-v4' in the Gym library, abstracts the game's mechanics into observations, actions, and rewards, which an AI agent can interact with.\n",
    "In this setup, the agent observes the game state (pixel data from the screen), selects actions (moving the paddle left or right), and receives rewards (the score for breaking bricks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the Agent\n",
    "\n",
    "The following code configures and runs the training of the DQN. (This process can take many hours if you increase a lot the number of environment steps to run during training - TIMESTEPS). This code updates the loss and average return. The losses reported reflect the average loss for individual training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Aq8VWUBFz5_t"
   },
   "outputs": [],
   "source": [
    "# VERSION 1 of the DQN\n",
    "import ale_py  # Import this before making any Atari environments\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# Choose the game name\n",
    "GAME_NAME = 'Breakout'  # Or 'Atlantis'\n",
    "\n",
    "# Create the game environment, note that we wrap it with VecFrameStack for preprocessing\n",
    "env_id = f\"{GAME_NAME}NoFrameskip-v4\" # pick desired game version\n",
    "env = make_atari_env(env_id, n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Initialize the DQN to use a CNN model as feature extractor\n",
    "model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=\"./atari_dqn_tensorboard/\")\n",
    "\n",
    "# Load your previously trained model to continue training where you stopped\n",
    "# set to TRUE and make sure the model_path is correct to load a previously trained model\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    model_path = f\"{GAME_NAME}_dqn_model.zip\"\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "\n",
    "# Train the agent\n",
    "TIMESTEPS = 1e5\n",
    "model.learn(total_timesteps=TIMESTEPS, progress_bar=True)\n",
    "\n",
    "# Save the model\n",
    "model.save(f\"{GAME_NAME}_dqn_model\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Don't forget to close the environment when you are done\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tNSIXs5BgN3"
   },
   "outputs": [],
   "source": [
    "# VERSION 2 of the DQN (including your variations)\n",
    "# - remember to SAVE THE SAMPLE VIDEO for VERSION 1 with the code below before running your new version of the DQN!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "## Videos\n",
    "\n",
    "The following functions allow us to watch the agent play the game in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_Qpx5qvI3wD0"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import os\n",
    "\n",
    "# Set the game name here\n",
    "GAME_NAME = 'Breakout'  # If you prefer, you can try the 'Atlantis' game as well\n",
    "\n",
    "# Load your previously trained model\n",
    "model_path = f\"{GAME_NAME}_dqn_model.zip\"\n",
    "#model = PPO.load(model_path)\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Create the Atari environment and apply the correct wrappers\n",
    "env_id = f\"{GAME_NAME}NoFrameskip-v4\"\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Record the environment\n",
    "video_folder = '/content/videos'\n",
    "if not os.path.exists(video_folder):\n",
    "    os.makedirs(video_folder)\n",
    "\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                       record_video_trigger=lambda step: step == 0,\n",
    "                       video_length=500,\n",
    "                       name_prefix=f\"{GAME_NAME}-agent\")\n",
    "\n",
    "# Reset the environment and observe the initial observation shape\n",
    "obs = env.reset()\n",
    "print(\"Initial observation shape:\", obs.shape)  # Should be (1, 84, 84, 4)\n",
    "\n",
    "# Run one episode\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "# Close the environment which should also save the video\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YTTIWddpAawT"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Load the video and encode it\n",
    "video_path = '/content/videos/'  # Make sure this matches the path where the videos are saved\n",
    "video_files = [f for f in os.listdir(video_path) if f.endswith('.mp4')]\n",
    "\n",
    "if video_files:\n",
    "    video_filename = video_files[-1]  # if you expect multiple videos, modify this to select the correct one\n",
    "    full_video_filename = f\"{video_path}/{video_filename}\"\n",
    "    mp4 = open(full_video_filename, 'rb').read()\n",
    "    encoded = b64encode(mp4).decode('ascii')\n",
    "    html = HTML(data=f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\"></video>')\n",
    "else:\n",
    "    html = HTML(data=\"Error: No video found\")\n",
    "\n",
    "html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hSbvd8r15UNr"
   },
   "outputs": [],
   "source": [
    "# RUN this command to visualize tensorboard logs stored\n",
    "# More info on what has been logged here: https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/atari_dqn_tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjbA8y3OwZnJ"
   },
   "source": [
    "## **Question 1**:\n",
    "Train a different model than the provided one (if possible, a similar number of iterations).\n",
    "\n",
    "For this new version, you need to **change at least two elements** related to any of the following: the CNN used, the policy, other agent configuration parameters or model compilation options (The number of training iterations does not count ;-).\n",
    "Check the documentation of the implementation of DQN used here:\n",
    "[Stable_Baselines3 DQN documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#module-stable_baselines3.dqn)\n",
    "\n",
    "**Briefly explain your variation and if you have an intuition of how they could affect the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6slSdNb_Ffv"
   },
   "outputs": [],
   "source": [
    "# if you donÂ´t know where to start, you can think about modifying these parameters when defining the DQN model\n",
    "#exploration_fraction (float) â fraction of entire training period over which the exploration rate is reduced\n",
    "#exploration_initial_eps (float) â initial value of random action probability\n",
    "#exploration_final_eps (float) â final value of random action probability\n",
    "#print(model.exploration_final_eps, model.exploration_initial_eps, model.exploration_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o45CRwEwwb6-"
   },
   "source": [
    "ANSWER: [YOUR-ANSWER-HERE] (max 5 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMHT5REfwlWs"
   },
   "source": [
    "## **Question 2**:\n",
    "Save and visualize the video for the two trained models and log/save the main statistics you think are useful (you most likely already have everything you need in the tensorboard folder logs and in the printed information in the cell output during training)\n",
    "\n",
    "**Which model do you think is better? Which metric/s or information are you using to decide this?** *Even if your models are not good, neither one of them, think about which information you would need to compare two solutions for this problem*\n",
    "(More ideas about questions you can try to answer in the discussion: Do you think the videos are enough to evaluate the method? What parameters or statistics you may find useful from the logged information?)\n",
    "\n",
    "**Do you think it is better/necessary to run the model with a different configuration during final evaluation than during training? Can you investigate which one?** You can write additional code if you need it for your evaluation tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92XxqbR7wi8a"
   },
   "source": [
    "ANSWER: [YOUR-ANSWER-HERE] (max 10 lines)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": [
    {
     "file_id": "1FMNZ-8a1JygoiJUyUW7sCaL948HVYqH0",
     "timestamp": 1732666974065
    },
    {
     "file_id": "https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_4_atari.ipynb",
     "timestamp": 1732658521290
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
